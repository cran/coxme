\section{lmekin}
The original kinship library had an implementation of linear mixed effects
models using the matrix code found in coxme.  
The reason for the program was entirely to check our arithmetic: it should
get the same answers as lme.
With more time and a larger test suite the routine is no longer 
necessary for this purpose, and I intended to retire it.
However, it had became popular with 
users since it can fit a few models that lme cannot, so now is a permanent
part of the package.

The orignal code was based on equation 2.14 of Pinheiro and Bates, the one
they do not recommmend for computation.
This is a quite sensible formula if there is a \emph{single}
random effect but it does not generalize well.
This release follows the compuational strategy of lme much more closely.
Note that a lot of the code below is a pure copy of the coxme code.

<<lmekin>>=
lmekin <- function(formula,  data, 
        weights, subset, na.action,
        control, varlist, vfixed, vinit, 
        method=c("ML", "REML"),
        x=FALSE, y=FALSE, model=FALSE,
        random, fixed, variance,  ...) {

    Call <- match.call()
    sparse <- c(1,0)  #needed for compatablily with coxme code
    <<lme-process-standard-arguments>>
    <<decompose-lme-formula>>
    <<build-control-structures>>
    <<lmekin-compute>>
    <<lmekin-finish-up>>
    }
<<lmekin-helper>>
@ 

The standard argments processing is copy of that for coxme, but with
the word ``lmekin'' in error messages.

<<lme-process-standard-arguments>>=
if (!missing(fixed)) {
    if (missing(formula)) {
        formula <- fixed
        warning("The 'fixed' argument of lmekin is depreciated")
        }
    else stop("Both a fixed and a formula argument are present")
    }
if (!missing(random)) {
    warning("The random argument of lmekin is depreciated")
    if (!inherits(random, 'formula') || length(random) !=2) 
        stop("Invalid random formula")
    j <- length(formula)   #will be 2 or 3, depending on if there is a y

    # Add parens to the random formula and paste it on
    formula[[j]] <- call('+', formula[[j]], call('(', random[[2]]))  
    }

if (!missing(variance)) {
    warning("The variance argument of lmekin is depreciated")
    vfixed <- variance
    }

method <- match.arg(method)

temp <- call('model.frame', formula= subbar(formula))
for (i in c('data', 'subset', 'weights', 'na.action'))
    if (!is.null(Call[[i]])) temp[[i]] <- Call[[i]]
m <- eval.parent(temp)

Y <- model.extract(m, "response")
n <- length(Y)
if (n==0) stop("data has no observations")

weights <- model.weights(m)
if (length(weights) ==0) weights <- rep(1.0, n)
else if (any(weights <=0))
    stop("Negative or zero weights are not allowed")

offset <- model.offset(m)
if (length(offset)==0) offset <- rep(0., n)

# Check for penalized terms; the most likely is pspline
pterms <- sapply(m, inherits, 'coxph.penalty')
if (any(pterms)) {
    stop("You cannot have penalized terms in lmekin")
    }

if (missing(control)) control <- lmekin.control(...)
@ 

Get the X-matrix part of the formula.
This is parallel to the version in coxme, the main difference is
that we keep the intercept term.
We check for the cluster and strata terms because it is a mistake
that I anticipate users to make.
<<decompose-lme-formula>>=
flist <- formula1(formula)
if (hasAbar(flist$fixed))
    stop("Invalid formula: a '|' outside of a valid random effects term")

special <- c("strata", "cluster")
Terms <- terms(flist$fixed, special)
if (length(attr(Terms, "specials")$strata))
    stop ("A strata term is invalid in lmekin")
if (length(attr(Terms, "specials")$cluster))
    stop ("A cluster term is invalid in lmekin")
X <- model.matrix(Terms, m)
@
    
Now for the actual compuation.
We want to solve
\begin{align*}
  y & = X\beta + Zb + \epsilon \\
  b & \sim N(0, \sigma^2K) \\
  \epsilon &\sim N(0, \sigma^2)
\end{align*}
where $K$ is the variance matrix returned by [[kfun]].
If we know $K$, one way to solve this is as an augmented
least squares problem with
\begin{equation*}
  y^*=\left(\begin{array}{c} y\\0 \end{array} \right) \qquad
  X^*=\left(\begin{array}{c} X\\0 \end{array} \right) \qquad
  Z^*=\left(\begin{array}{c} Z\\ \Delta  \end{array} \right)
\end{equation*}
where $\Delta' \Delta= K^{-1}$.                     
The dummy rows of data have $y=0$, $X=0$ and $\Delta$ as
the predictor variables.
With known $\Delta$, this gives the solution to all the other
parameters as an ordinary least squares problem.
If $K= U'U$ for $U$ an upper triangular matrix, then 
$K^{-1} = L'L$ where $L= (U')^{-1}= (U^{-1})'$ is lower triangular.
Then 
\begin{align}
  \Delta' \Delta &= K^{-1} \nonumber \\
 \Delta &= L \label{eq:delta}
\end{align}

In our case $K$ will be the iteration target of the
[[optim]] function, and we need to 
evaluate the other parameters in order to determine the log-likelihood.
In coxme this is done inside a C routine, here we can use the
more direct method.
In the original [[lmekin]] function we made the assumption that
$Z$ was an identity matrix, which allowed for a simple solution
using only the generalized cholesky decompostion found in the
[[bdsmatrix]] library.
Here we use the more general QR method as outlined in Pinheiro and
Bates.  
Assume that $Z$ has $q$ columns and $X$ has $p$ columns, the number
of random and fixed coefficients, respectively.
Then
\begin{align*}
  \left(Z^*, X^* \right) &= QR \\
  R &= \left( \begin{array}{cc} R_{11}& R_{12} \\ 0& R_{22} \\
                                0 & 0
    \end{array} \right) \\
  Q'y &= \left( \begin{array}{c} c_1 \\ c_2 \\ c_3 \end{array} \right)
\end{align*}                                                         %'

The orthagonal matrix $Q$ is $n\times n$, $R_{11}$ is $q \times q$ and upper 
triangular, $R_{22}$ is $p \times p$ upper triangular, and $R$ is $n \times p$.
The vectors $c_1$, $c_2$, and $c_3$ are of lengths $q$, $p$, and
$n- (p+q)$, respectively.
Using slightly different notation, Pinheiro and Bates show
that the solution vector and the profiled log-likelihood are (equations 2.19
and 2.21)
\begin{align}
  \hat\beta(\theta) &= R_{22}^{-1} c_2  \label{bhat}\\
  \hat\sigma^2(\theta) &= ||c_3||^2/n  \label{sighat}\\
  \log(L(\theta)) &= \frac{n}{2}\left[ \log n - \log(2\pi) -1 \right] -
    n\log |c_2| + \log\left( {\rm abs} \frac{|\Delta|}{|R_{11}} \right) 
     \label{loghat}
\end{align}
Here $|c|$ is the norm of a vector $c$  and $|A|$ the determinant of a matrix
$A$.  The determinant of a triangular matrix is the product of its
diagonal elements.  The solution for $\hat\beta$ is returned by the
[[qr.coef]] routine.

The restricted maximum likelihood estimate (REML) follows from the same
decompostions, but with
\begin{align}
  \hat\sigma^2_{REML}(\theta) &= ||c_3||^2/(n-p)  \label{sighatREML}\\
  \log(L(\theta))_{REML} &= \frac{n}{2}\left[\log n - \log(2\pi) -1\right] -
    (n-p)\log |c_2| + \log \left( {\rm abs} \frac{|\Delta|}{|R|} \right) 
     \label{loghatREML}
\end{align}


<<lmekin-compute>>=
<<define-penalty>>
<<define-xz>>
<<lmekin-fit>>
@ 

The define-penalty code is shared with coxme, it defines the
function [[kfun]] which returns $K/\sigma^2$ given the parameters $\theta$.
The next bit of code defines $X^*$ and the top portion of $Z^*$
as sparse Matrix objects.
The definition $X^*$ is easy as we already have it in hand.  
For $Z^*$ most of the work is creating the design matrix for the
intercepts from our very compressed form [[fmat]].
That matrix has one column for each unique factor and $n$ rows,
each column contains the coefficent mapping of subjects to coefficients.
So for instance assume 6 subject and a term of [[(1|group)]] with 3
groups.
The corresponding column of fmat might be (1,2,2,3,1,3) showing that
subject 1 is in group 1, subject 1 in group 2, etc.  
In a genetic data set with kinship each subject would be in thier
own group and the column would be some permutation of 1:n.
The corresponding design matrix is

\begin{equation*}
  \left( \begin{array}{cccccc}
    1&0&0 \\ 0&1&0 \\ 0&1&0 \\
    0&0&1 \\ 1&0&0 \\0&0&1 \end{array} \right)
\end{equation*}

The sparse coding of this for a dgCMatrix object has 
components
\begin{description}
  \item[i] a vector containing all the row numbers of the non zero
    elements, with rows numbered from zero.
    In this case it would be 0, 4, 1, 2, 3, 5.
  \item[p] a vector with first element 0 such that diff(p) = the
    number of non-zeros in each column
  \item[x] the values of the non-sparse elements
  \item[Dim] dimensions of the matrix
  \item[Dimnames] optional dimnames
  \item[factors] an empty list, used by later Matrix routines for
    factorization information
\end{description}

The variable [[zstar1]] is the top part of $Z^*$, i.e., the full $Z$
matrix, in sparse form.  
At each iteration $\Delta$ changes, we splice that on at that time.
If we use a decompostion of $(Z^*, X^*)$ as defined above there
is a problem: the sparse QR routine will rearrange the columns of
the decomposed matrix so as to be most efficient (some permutataions
retain more sparseness than others). 
This is ok as long as the rearrangement does not intermix $Z$ and
$X$, and in fact it often does not since $Z$ will be ``sparser'' than $X$ 
in most problems; but we can't guarrantee it.                  %'
Thus we do a two-step decompostion:
\begin{align*}
  (Z,X) &= (Q_1 | Q_2) \left(\begin{array}{cc} R_1 & A\\ 0 & R2 
                       \end{array} \right) \\
    Q_1'X &= \left ( \begin{array}{c} A\\ Q_2R_2 \end{array} \right)
\end{align*}
Thus $Q_1$ and $R_1$ are the result of a QR decomposition of $Z$,
and $Q_2$, $R_2$ from a QR decompostion of the
the lower rows of $Q_1'X$.
The final result is the same as a single QR call for the combined
matrix.

At the time of this writing the Matrix library's [[qr.qty]] routine  %'
could not deal with a sparse matrix as the second argument,
thus in creating [[xstar]] below we force a non-sparse version.
This is not a computational problem since X is always of modest size,
it is the random effects matrix Z which can be huge and for which
sparseness can pay off handsomely.
The Matrix routine by
default uses a sparse form if the object has over 1/2 zeros,
which would be true for some $X$ matrices.
<<define-xz>>=
#Define Z^* and X^*
itemp <- split(row(fmat), fmat)
zstar1 <- new("dgCMatrix", 
              i= as.integer(unlist(itemp) -1),
              p= as.integer(c(0, cumsum(unlist(lapply(itemp, length))))),
              Dim=as.integer(c(n, max(fmat))),
              Dimnames= list(NULL, NULL),
              x= rep(1.0, length(fmat)),
              factors=list())
if (length(zmat) >0)  {
    # there were random slopes as well
    zstar1 <- cbind(zstar1, as(Matrix(zmat), "dgCMatrix"))
}

nfrail <- ncol(zstar1)
nvar <- ncol(X)
if (nvar == 0)  xstar <- NULL  #model with no covariates
else xstar <- rbind(Matrix(X, sparse=FALSE),
                    matrix(0., nrow=nfrail, ncol=ncol(X)))
ystar <- c(Y, rep(0.0, nfrail))
@ 

Now to do the fit. 
Define logfun, which returns the loglik (without the constant terms) for
a given trial value of theta.
Use a gridsearch to find the best starting values, and start the
optim() routine there.
The convergence criterion for optim works well if the true minimum is
around 1 in absolute value; our last line of logfun makes that true 
if the starting estimate is exactly the final solution.
Notice that the max for $\theta$ only depends on the loglik,
equation \ref{loghat} or \ref{loghatREML}.

For the ML estimate \ref{loghat} we need the determinant of $R_{11}$
The documentation for the qr routine in the Matrix library has an
unclear reference to column permutations (it says they can exist, but
not how to turn this on or off nor the default).  Since we need to
keep $Z$ before $X$ the code below has 2 calls, first on the $Z$
portion and then on the transformed $X$ portion.

A second nuisance is that the qr.R function in the Matrix
library insists on printing a warning message about the fact
that permutations may exist.  For computation of a determinant,
which is the product of the diagonal elements of R,
any reordering is irrelevant to us.  We use a local function
mydiag to work around this.

To create $Delta$ first do the cholesky decompostion, which returns
the upper triangular matrix $U$ by default.
The solve function when applied to the cholesky uses a fast backsolve
approach. (But you can't use the backsolve function, since the Matrix
library didn't have a method for it at this time.  I originally tried
this to make the code clearer wrt to the algorithm.)
Per equaton \eqref{eq:delta} we need to transpose the result to
lower triangular form.

<<lmekin-fit>>=
mydiag <- function(x) {
    if (inherits(x, "sparseQR")) diag(x@R)
    else diag(qr.R(x))
}

logfun <- function(theta, best=0) {
    vmat <- kfun(theta, varlist, vparm, ntheta, ncoef)
    Delta <- t(solve(chol(as(vmat, "dsCMatrix"), pivot=FALSE)))
    zstar <- rbind(zstar1, Delta)
    qr1 <- qr(zstar)
    dd <- mydiag(qr1) 
    cvec <- as.vector(qr.qty(qr1, ystar))[-(1:nfrail)]  #residual part
    if (nvar >0) {  # have covariates
        qr2 <- qr(qr.qty(qr1, xstar)[-(1:nfrail),])
        cvec <- qr.qty(qr2, cvec)[-(1:nvar)]  #residual part
        if (method!= "ML") dd <- c(dd, mydiag(qr2))
    }

    loglik <- sum(log(abs(diag(Delta)))) - sum(log(abs(dd)))
    if (method=="ML") loglik <- loglik - .5*n*log(sum(cvec^2))
    else              loglik <- loglik - .5*length(cvec)*log(sum(cvec^2))
    
    best - (loglik+1)  #optim() wants to minimize rather than maximize
}

nstart <- sapply(itheta, length)
if (length(nstart) ==0) theta <- NULL #no thetas to solve for
else {
    #iteration is required
    #make a matrix of all possible starting estimtes
    testvals <- do.call(expand.grid, itheta)
    bestlog <- NULL
    for (i in 1:nrow(testvals)) {
        ll <- logfun(as.numeric(testvals[i,]))
        if (is.finite(ll)) {
            #ll calc can fail if someone picks a very bad starting guess
            if (is.null(bestlog) || ll < bestlog) {  
                # (optim is set up to minimize)
                bestlog <- ll
                theta <- as.numeric(testvals[i,])
            }
        }
    }
    if (is.null(bestlog))
        stop("No starting estimate was successful")

    optpar <- control$optpar
    optpar$hessian <- TRUE
    mfit <- do.call('optim', c(list(par= theta, fn=logfun, gr=NULL,
                                    best=bestlog), optpar))
    theta <- mfit$par
}
@ 

At this point the optimal $\theta$ has been found.
Now do one more pass with the ``internals'' of the logfun
function, and compute other quantities that we didn't need    
for the intermediate iterations.
Remember that lmekin was designed for genetic problems,
and for these $Z$ will be very large and sparse while $X$ will
be modest. 
<<lmekin-compute>>=
vmat <-  kfun(theta, varlist, vparm, ntheta, ncoef)
Delta <- t(solve(chol(as(vmat, "dsCMatrix"), pivot=FALSE)))
zstar <- rbind(zstar1, Delta)
qr1 <- qr(zstar)
dd <- mydiag(qr1)
ctemp <- as.vector(qr.qty(qr1, ystar))
cvec <- ctemp[-(1:nfrail)]  #residual part

if (is.null(xstar)) { #No X covariates
    rcoef <- qr.coef(qr1, ystar)
    yhat <- qr.fitted(qr1, ystar)
}
else {
    qtx <- qr.qty(qr1, xstar)
    qr2 <- qr(qtx[-(1:nfrail),,drop=F])
    if (method!="ML") dd <- c(dd, mydiag(qr2))

    fcoef <-qr.coef(qr2, cvec)
    yresid <- ystar - xstar %*% fcoef
    rcoef <- qr.coef(qr1, yresid)
    cvec <- qr.qty(qr2, cvec)[-(1:nvar)] #residual part
    if (inherits(qr2, "sparseQR")) varmat <- chol2inv(qr2@R)
    else varmat <- chol2inv(qr.R(qr2))
    yhat <- as.vector(zstar1 %*% rcoef + X %*% fcoef) #kill any names
}

if (method=="ML") {
    sigma2 <- sum(cvec^2)/n  #MLE estimate  
    loglik <- sum(log(abs(diag(Delta)))) - 
          (sum(log(abs(dd))) + .5*n*(log(2*pi) +1 + log(sigma2)))
}
else {
    np <- length(cvec)  # n-p
    sigma2 <- mean(cvec^2)  # divide by n-p
    loglik <- sum(log(abs(diag(Delta)))) -
        (sum(log(abs(dd))) + .5*np*(log(2*pi) + 1+ log(sigma2)))
}
	
# Debugging code, set the argument to TRUE only during testing
if (FALSE) {
    # Compute the alternate way (assumes limited reordering)
    zx <- cbind(zstar, as(xstar, class(zstar)))
    qr3 <- qr(zx)
    cvec3 <- qr.qty(qr3, ystar)[-(1:(nvar+nfrail))]
    if (method=="ML")  dd3 <- (diag(myqrr(qr3)))[1:nfrail]
    else               dd3 <- (diag(myqrr(qr3)))[1:(nfrail+nvar)]
    #all.equal(dd, dd3)
    #all.equal(cvec, cvec3)
    acoef <- qr.coef(qr3, ystar)
    browser()
}
@ 

Bundle the results together into an output object.
This object differs from the old lme object, having both more and less
information.  
First we call the wrapup functions to retransform any parameters.
At this point we also rescale the other variance components: the iteration
used $\sigma^2 \Sigma$ as the variance matrix for the random effects,
the user wants to think of $\Sigma$ as the product of these.
<<lmekin-finish-up>>=

newtheta <- random.coef <- list()  
nrandom <- length(varlist)
sindex <- rep(1:nrandom, ntheta) #which thetas to which terms
bindex <- rep(1:nrandom, rowSums(ncoef)) # which b's to which terms
for (i in 1:nrandom) {
    temp <- varlist[[i]]$wrapup(theta[sindex==i], rcoef[bindex==i], 
                                vparm[[i]])
    newtheta <- c(newtheta, lapply(temp$theta, function(x) x*sigma2))
    if (!is.list(temp$b)) {
        temp$b <- list(temp$b)
        names(temp$b) <- paste("Random", i, sep='')
        }
    random.coef <- c(random.coef, temp$b)
    }
@ 

We create a variance matrix only for the fixed effects.
The primary reason is that even though $Z$ is sparse the variance
matrix associated with $Z$ will usually be dense; for many
of our genetics problems this would easily drive R out of
memory.
If the columns of $(Z,X)$ remain in order we only need to invert
the lower triangle of $R$, but if they have been permuted we
need to force separation between $Z$ and $X$ by doing the 
decompostion in two steps.
<<lmekin-finish-up>>=

if (length(fcoef) >0) {
    # There are fixed effects
    nvar <- length(fcoef)
    fit <- list (coefficients=list(fixed=fcoef, random=random.coef),
                 var = varmat * sigma2,
                 vcoef =newtheta,
                 residuals= Y- yhat,
                 method=method,
                 loglik=loglik,
                 sigma=sqrt(sigma2),
                 n=n,
                 call=Call)
}
else fit <- list(coefficients=list(fixed=NULL, random=random.coef),
                 vcoef=newtheta,
                 residuals=Y - yhat,
                 method=method,
                 loglik=loglik,
                 sigma=sqrt(sigma2),
                 n=n,
                 call=Call)

if (!is.null(theta)) {
    fit$rvar <- mfit$hessian
    fit$iter <- mfit$counts
}
if (x) fit$x <- X
if (y) fit$y <- Y
if (model) fit$model <- m

na.action <- attr(m, "na.action")
if (length(na.action)) fit$na.action <- na.action
                         
class(fit) <- "lmekin"
fit
@ 

And last, a couple of helper functions
<<lmekin-helper>>=
residuals.lmekin <- function(object, ...) {
    if (length(object$na.action)) naresid(object$.na.action, object$residuals)
    else object$residuals
}
@ 
