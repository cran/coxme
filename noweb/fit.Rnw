\section{Fitting}
Consider the basic model
\begin{eqnarray*}
 \lambda(t) &=& \lambda_0(t) e^{X\beta + Zb} \\
 b &\sim& N(0, \Sigma(\theta))
\end{eqnarray*}

There are two sets of parameters.
The first is the set of regression coefficient $\beta$ and $b$,
the second is the vector $\theta$ that determines the variance
structure.
The basic structure of the iteration is
\begin{itemize}
\item an outer iteration process for $\theta$ which uses the standard
S routine [[optim]]
\item for any given realization of $\theta$ a computation of the optimal
values for $\beta$ and $b$
\begin{itemize}
\item S code is used to create the penalty matrix $\Sigma(\theta)$
\item C code solves for the regression coefficients, given $\Sigma$.
\end{itemize}
\end{itemize}

The overall outline of the routine is
<<coxme.fit>>=
    <<coxme-setup>>
    <<null-fit>>
    <<define-penalty>>
    <<coxfit6a-call>>
    <<coxme-fit>>
    <<coxme-finish>>
@ 

\subsection{Penalty matrix}
For the C code, the variance matrices of the individual
random effects are glued together into one
large bdsmatrix object $\Sigma$, [[kmat]] in the code; 
the inverse matrix $P = \Sigma^{-1}$ or 
[[ikmat]] is the penalty matrix of the computation,
is what is actually passed to C.
(The first large use of this code was for family correlation, where
$\Sigma$ is based on the \emph{kinship} matrix.  The variable names
[[kmat]] = $\Sigma$, [[ikmat]] for the inverse and [[kfun]] for
the calculation arise from this legacy.)
In order to make use of sparseness, 
the columns of [[kmat]] are expected to be in the following order
\begin{enumerate}
\item Random intercepts that are subject to sparse computation.
Only one random term is allowed to use sparse representation, i.e., the
first term in the model formula that has an intercept.
We have reordered the random terms, if necessary, to make it first in the list.
\item The remaining random intercepts
\item Other random coefficients (slopes)
\end{enumerate}
The overall coefficient vector has the random effects $b$ followed
by the fixed effects $\beta$,
with $b$ in the same order as the penalty matrix.

The key code chunk below creates kmat given the parameter vector $\theta$
([[theta]] for the non-mathematics types) and the variance list information.
Each of the [[generate]] functions creates either an ordinary matrix or
one represented in \emph{b}lock \emph{d}iagonal \emph{symmetric} form,
which consists of a block diagonal portion in the upper left bounded by a
dense portion on the right.
(A bdsmatrix with only one diagonal block is dense, one with many blocks
will be sparse.)
The C code expects a single bdsmatrix, so any term after the first is
is added to the dense portion of the first matrix.
It also expects the bdsmatrix to have at least one ``block'', and that
block must involve no more than the first column of $F$.
If the first term is a simple matrix, we just split off its first
element.
<<define-penalty>>=
kfun <- function(theta, varlist, vparm, ntheta, ncoef) {
    nrandom <- length(varlist)
    sindex <- rep(1:nrandom, ntheta) #which thetas to which terms

    tmat <- varlist[[1]]$generate(theta[sindex==1], vparm[[1]]) 
    dd <- dim(tmat)
    if (length(dd) !=2 || any(dd != rep(ncoef[1,1]+ncoef[1,2], 2)))
        stop("Incorrect dimensions for generated penalty matrix, term 1")
    if (!inherits(tmat, 'bdsmatrix')) 
        tmat <- bdsmatrix(blocksize=integer(0), blocks=numeric(0), rmat=tmat)
@ 
If there is only a single random term, then our work is done.
Otherwise we have some nit-picky bookkeeping.  Not
particularly hard but a nuisance.
Say for example that there are 3 terms with the following structure
$$\begin{tabular}{rccc}
& sparse & non-sparse \\
& intercept & intercept & covariate \\ \hline
term 1 & 60 & 2 & 64 \\
term 2 &  0 & 5 & 0 \\
term 3 &  0 & 8 & 16 \\
\end{tabular}
$$
Here ``sparse'' means precisely the block-diagonal portion of the
returned variance matrix.
This corresponds to [[~ (1+x | g1) + (1|g2) + (1+ z1 + z2 | g3)]]
where [[g1]] has two common and 60 uncommon levels, [[g2]] has 5
levels and [[g3]] has 8.
(A complicated random effects model I admit.)
The first variance matrix is required to be of a [[bsdmatrix]]
form with an [[rmat]] slot of dimension $126 \times 66$, call this $T$.
The variance structure for the other two terms, call them $U$ and
$V$, can be of any matrix type.
The final bdsmatrix will have the block-diagonal portions for the
first 60 elements
and an overall right-hand side matrix of the form
$$
R= \left( \begin{array}{ccccc}
             T[1-62, 1-2] & 0 & 0 &T[1-62, 3-66] &0\\
              0 & U &0 & 0 & 0 \\
              0 & 0 & V[1-8, 1-8] &0 & V[1-8, 9-24]\\
              0 & 0 & 0 & T[63-128, 3-66] & 0 \\
              0 & 0 & 0 & 0 & V[9-24,9-24] \\ \end{array} \right)
$$

First we set up the total number of rows and columns of R,
then march through the matrix.  
We need to first process the non-sparse rows of the first variance
matrix [[tmat]]; if that contains a substantial number of sparse
columns then it is important to subset \emph{before} creating a
regular matrix from the remainder; the construction
[[(as.matrix(tmat))[k,k]]] would create a temporary matrix of possibly
vast proportions.  
At this point in time all of the intercepts map before any covariates
so we can keep separate indices for the intercept-rows-so-far [[indx1]] and
covariate-rows-so-far [[indx2]], with the second one starting after the
end of all the intercepts.  
What we are doing is in essence a diagonal bind of matrices, pasting
blocks down the diagonal, but S has no [[dbind]] function.
<<define-penalty>>=    
    if (nrandom ==1) return(tmat)

    # Need to build up the matrix by pasting up a composite R
    nsparse <- sum(tmat@blocksize)
    nrow.R <- sum(ncoef)
    ncol.R <- nrow.R - nsparse
    R <- matrix(0., nrow.R, ncol.R)
    indx1 <- 0               #current offset  wrt filling in intercepts
    indx2 <- sum(ncoef[,1])  #current offset  wrt filling in slopes
    
    if (ncol(tmat) > nsparse) { #first matrix has an rmat component
        k <- (nsparse+1):ncol(tmat)
        temp <- as.matrix(tmat[k,k])

        if (ncoef[1,1] > nsparse) { #intercept contribution to rmat
            j <- ncoef[1,1] - nsparse   #number of intercept columns
            R[1:nrow(temp), 1:j] <- temp[,1:j]
            indx1 <- indx1 +j

            if (ncoef[1,2] >0) { #copy correlation with intercept
                k <- 1:ncoef[1,2]
                R[1:ncoef[1,1], indx2+k-nsparse] <- temp[1:ncoef[1,1], j+k]
                }
            }
        else j <- 0
        
        if (ncoef[1,2] >0) { #has a slope contribution to rmat
            k <- 1:ncoef[1,2]
            R[indx2+k, indx2+k -nsparse] <- temp[ncoef[1,1]+k, j+k]
            indx2 <- indx2 + ncoef[1,2]
            }
        }
    
    for (i in 2:nrandom) {
        temp <- as.matrix(varlist[[i]]$generate(theta[sindex==i], vparm[[i]]))
        if (any(dim(temp) != rep(ncoef[i,1]+ncoef[i,2], 2)))
            stop(paste("Invalid dimension for generated penalty matrix, term",
                       i))
        
        if (ncoef[i,1] >0)  { # intercept contribution
            j <- ncoef[i,1]
            R[indx1 +1:j, indx1 +1:j-nsparse] <- temp[1:j,1:j]
            indx1 <- indx1 + j
            
            if (ncoef[i,2] >0) {
                k <- 1:ncoef[i,2]
                R[indx1+1:j, indx2 +k -nsparse] <- temp[1:j, k+ j]
                R[indx2+k, indx2 +k -nsparse] <- temp[k+j, k+j]
                }
            }
        else if (ncoef[i,2]>0) {
            k <- 1:ncoef[i,2]
            R[indx2+k, indx2+k -nsparse] <- temp
            }
        indx2 <- indx2 +ncoef[i,2]
        }
    bdsmatrix(blocksize=tmat@blocksize, blocks=tmat@blocks, rmat=R)
    }    
@ 

\subsection{C routines}
The C-code underlying the computation is broken into 3 parts.
This was done for memory efficiency; due to changes in R and
S-Plus over time it may not as wise an idea as I once thought,
this is an obvious area for future simplification.

The initial call passes in the data, which is then copied to
local memory (using calloc, not under control of S memory
management) and saved.  The parameters of the call are
\begin{description}
  \item[n] number of observations
  \item[nvar] number of fixed covariates in X 
  \item[y] the matrix of survival times.  It will have 2 columns for normal
    survival data and 3 columns for (start, stop) data
  \item[x] the concatenated Z and X matrices
  \item[offset] vector of offsets, usually 0
  \item[weights] vector of case weights, usually 1
  \item[newstrat] a vector that marks the end of each stratum.  If for 
    instance there were 4 strata with 100 observations in each, this vector
    would be c(100,200,300,400); the index of the last observation in each.
  \item[sorted] A matrix giving the order vector for the data.  The first 
    column orders by strata, time within strata (longest first), and status
    within time (censored first).  For start, stop data a second column orders
    by strata, and entry time within strata. The -1 is because subscripts 
    start at 1 in S and 0 in C.
  \item[imap] matrix containing the indices for random intercepts.  
  \item[findex] a 0/1 matrix with one column for each of fcol and nfrail
    rows, which marks which coefficients of $b$ are a part of that set.
    (A bookkeeping array for the C code that is easier to create here.)
  \item[$P$] some parameters of the bdsmatrix representing the penalty  
\end{description}
The other control parameters are fairly obvious.  From this data the C
routine can compute the total number of penalized terms and the number that
are sparse from the structure of the bdsmatrix, and the total number of
intercept terms as max(imap).  Other dimensions follow from those.

A dummy call to [[kfun]] gives the necessary sizes for the penalty matrix.
All columns of the stored $X$ matrix are centered and scaled, and these
factors are returned.  
For [[theta]] we use the first element of each set of initial values found
in [[itheta]].

<<coxfit6a-call>>=
if (length(itheta) >0) theta <- sapply(itheta, function(x) x[1])
else theta <- numeric(0)
dummy <- kfun(theta, varlist, vparm, ntheta, ncoef)
if (is.null(dummy@rmat)) rcol <- 0
    else                 rcol <- ncol(dummy@rmat)
npenal <- ncol(dummy)  #total number of penalized terms

if (ncol(imap)>0) {
    findex <- matrix(0, nrow=sum(ncoef), ncol=ncol(imap))
    for (i in 1:ncol(imap)) findex[cbind(imap[,i], i)] <- 1
    }
else findex <- 0  # dummy value

if (is.null(control$sparse.calc)) {
    nevent <- sum(y[,ncol(y)])
    if (length(dummy@blocksize)<=1) nsparse<- 0
    else nsparse <- sum(dummy@blocksize)
    itemp <- max(c(0,imap)) - nsparse  #number of non-sparse intercepts
    
    if ((2*n) > (nevent*(nsparse-itemp))) control$sparse.calc <- 0
    else control$sparse.calc <- 1
    }

ifit <- .C('coxfit6a', 
               as.integer(n),
               as.integer(nvar),
               as.integer(ncol(y)),
               as.double(c(y)),
               as.double(cbind(zmat,x)),
               as.double(offset),
               as.double(weights),
               as.integer(length(newstrat)),
               as.integer(newstrat),
               as.integer(sorted-1),
               as.integer(ncol(imap)),
               as.integer(imap-1),
               as.integer(findex),
               as.integer(length(dummy@blocksize)),
               as.integer(dummy@blocksize),
               as.integer(rcol),
               means = double(nvar),
               scale = double(nvar),
               as.integer(ties=='efron'),
               as.double(control$toler.chol),
               as.double(control$eps),
               as.integer(control$sparse.calc))
means   <- ifit$means
scale   <- ifit$scale
@ 

The second routine does the real work and is called within the [[logfun]]
function, which is the minimization target of [[optim]]. 
The function is called with a trial value of the variance parameters
$\theta$, and computes the maximum likelihood estimates of $\beta$ and $b$
for that (fixed) value of $\theta$, along with the penalized partial
likelihood.  
The normalization constants include the determinant of [[kmat]], but since
we are using Cholesky decompositions this can be read off of the diagonal.
Hopefully the coxvar routines have chosen a parameterization that will
mostly avoid invalid solutions, i.e., those where [[kmat]] is not
symmetric positive definite.

We found that it is best to always do the same number of iterations at
each call.  Changes in the iteration count (i.e. if one value of
$\theta$ requires 5 iterations to converge and another only 4 for instance) 
introduce little 'bumps' into the apparent loglik, which
drives [[optim]] nuts. Hence the min and max iteration count is identical.
A similar issue applies to the vector of starting estimates $(b, \beta)$.
It is tempting to use the final results from the prior $theta$ evaluation,
but again this introduces an artifact.
Thus all the calls to [[logfun]] use the same initial value.
Two obvious choices for [[init]] are a vector of zeros and the fit
to a fixed effects model.  The latter is likely to be better,
but I worry about cases where the fit is nearly
singular; user's sometimes fit models with more variables than they should. %'
The current compromise is .7*the final fit + .3*zeros; this number is no
more than a wild guess.
The addition of (1 -fit0) to the final logliklihood makes the 
the solution be in the neighborhood of 1 (for the case that the random
terms add nothing to the fit)
which works well with the convergence criteria of
the [[optim]] routine.

There are actually two C routines [[coxfit6b]] and [[agfit6b]], for ordinary
and (start,stop) survival data, respectively.  The [[ofile]] argument 
is a character string giving the choice.
<<define-logfun>>=
logfun <- function(theta, varlist, vparm, kfun, ntheta, ncoef, 
                   init, fit0, iter, ofile) {
    gkmat <- gchol(kfun(theta, varlist, vparm, ntheta, ncoef))
    ikmat <- solve(gkmat)  #inverse of kmat, which is the penalty
    if (any(diag(ikmat) <=0)) { #Not an spd matrix
        return(0)  # return a "worse than null" fit
        }
    fit <- .C(ofile,
              iter= as.integer(c(iter,iter)),
              beta = as.double(init),
              loglik = double(2),
              as.double(ikmat@blocks),
              as.double(ikmat@rmat),
              hdet = double(1))
    ilik <- fit$loglik[2] -
             .5*(sum(log(diag(gkmat))) + fit$hdet)
    -(1+ ilik - fit0)
    }
@ 


The third routine is used for iterative refinement of the Laplace
estimate. The arguments in this case are
\begin{description}
  \item[rfile] either 'coxfit6d' or 'agfit6d'
  \item[beta] the final solution vector $(b, \beta)$, though only $\beta$
    is used.
  \item[bmat] matrix of trial values for the random coefficients.
    Should have nfrail rows and refine.n columns.
  \item[loglik] log-likelihoods at the random points
\end{description}

The routine calculates the log-lik for a succession of Cox models, each
one using one of the random draws as it's random effect.       %'
The set of trial values is drawn from a t-distribution with
refine.df degrees of freedom, centered at the observed
random coefficients and with variance hmat-inverse.
Now if a random colum vector $X$ has the identity variance
matrix, then $CX$ have variance $CC'$.    %'
To get the variance we want we need a matrix
$C$ such that 
$CC' = (H_{bb})^{-1}$  where $H_{bb}$ is the portion of.%'
of the Hessian matrix $H$ corresponding to the random effects.
The [[bmat]] matrix below has such a random sample in each column.
We already have the cholesky decompostion $LDL'$ of $H$ in hand; %'
the decomposition of the upper left corner is the upper left
corner of the decompostion.
The inverse matrix is (L-inverse)' D-inverse (L-inverse), which %'
means we want to backsolve with respect to the upper triangular
portion. 

The natural way to generate t-variates is to use the mvtnorm
library; however, it expects $H^{-1}$ which may be a dense matrix,
we already have the sparse cholesky of $H$ [[hmat]], 
and so we essentially duplicate the lines of rmvt and dmvt
that occur after matrix decomposition.  For further details
see the vignette on laplace approximations.
<<refine>>=
if (refine.n > 0) {
    rdf <- control$refine.df
    nfrail <- ncol(gkmat)
    hmatb <- hmat[1:nfrail, 1:nfrail]
    #create the random t-variate with variance H-inverse
    bmat <- matrix(rnorm(nfrail*refine.n), ncol=refine.n)
    bmat <- backsolve(hmatb, bmat, upper=TRUE) /
        rep(sqrt(rchisq(refine.n, df=rdf)/rdf), each=nfrail)
    bmat2 <- bmat + fit$beta[1:nfrail]  #recenter

    rfit <- .C(rfile,
               as.integer(refine.n),
               as.double(fit$beta),
               as.double(bmat2),
               loglik = double(refine.n), dup=FALSE)

    # Penalty terms
    penalty1 <- colSums(bmat2*(ikmat %*% bmat2))/2
    penalty2 <- rowSums((t(bmat) %*% hmatb)^2 )/2

    # Constant for the Gaussian density,  and density of the t-dist (logs)
    gdens <- -0.5* (sum(log(diag(gkmat))) + nfrail*log(2* pi))
    logdet <- -sum(log(diag(hmatb)))
    tdens <- lgamma((nfrail + rdf)/2) - 
            (lgamma(rdf/2) + 0.5*(logdet + nfrail* log(pi*rdf) + 
             (nfrail+ rdf)* log(1 + 2*penalty2/rdf)))
    
    # Add it up, we have to be very careful about round-off
    n1 <- rfit$loglik + gdens - (penalty1 + ilik + tdens)
    n2 <- fit$loglik[2] + gdens - (penalty2 + ilik + tdens)
    temp <- max(n1, n2)  #scale so the largest value is about 1
    errhat <- (exp(n1-temp) - exp(n2-temp)) * exp(temp)
    #errhat <- (exp(rfit$loglik -(penalty1 + ilik)) - 
    #        exp(fit$loglik[2]- (penalty2 + ilik))) * exp(gdens-tdens)
    mtemp <- mean(errhat)             #estimated integral
    stemp <- sqrt(var(errhat)/refine.n)   #std of the estimate
    r.correct <- c(correction= log(1+ mtemp), std=stemp/(1 +mtemp)) #log scale
    #ilik <- ilik + r.correct[1]
}
@ 

The final routine [[coxfit6c]] is used for cleanup, and is
described in section \ref{sect:final}.

\subsection{Setup}
Preliminaries aside, let's now build the routine.               %'
The input arguments are as were set up by [[coxme]], this
routine would never be called directly by a user.
\begin{description}
  \item[x] the matrix of fixed effects
  \item[y] the survival times, an object of class 'Surv'
  \item[strata] strata vector
  \item[offset] vector of offsets, usually all zero
  \item[control] the result of a call to coxme.control
  \item[weights] vector of case weights. usually 1
  \item[ties] the method for handling ties, 'breslow' or 'efron'
  \item[rownames] needed for labeling the output, in the rare case that
    the X matrix is null.
  \item[imap] matrix of random factor (intercepts) indices.  If imap[4,1]=6,
    imap[4,2]=10 this means that observation 4 contributes to both coefficient
    6 and coefficient 10, both of which are random intercepts.
  \item[zmat] the Z matrix, the design matrix for random slopes
  \item[varlist] the list describing the structure of the random effects
  \item[vparm] the list of parameters for the variance functions
  \item[itheta] initial values for the random effects, e.g., the ones we need
    to solve for  (may be null if the variances are all fixed)
  \item[ntheta] vector giving the number of thetas for each random term
  \item[refine.n] number of iterations for iterative refinement
\end{description}
<<coxme-setup>>=
coxme.fit <- function(x, y, strata, offset, ifixed, control,
			weights, ties, rownames, 
			imap, zmat, varlist, vparm, itheta,
                        ntheta, ncoef, refine.n) {
    time0 <- proc.time()
    n <-  nrow(y)
    if (length(x) ==0) nvar <-0
    else nvar <- ncol(as.matrix(x))
    
    if (missing(offset) || is.null(offset)) offset <- rep(0.0, n)
    if (missing(weights)|| is.null(weights))weights<- rep(1.0, n)
    else {
	if (any(weights<=0)) stop("Invalid weights, must be >0")
	}
@ 
The next step is to get a set of sort indices, but not to actually
sort the data.  This was a key insight which allows the (start,stop)
version to do necessary bookkeeping in time of $(2n)$ instead of $O(n^2)$.
We sort by strata, time within strata (longest first), and status within
time (censor before deaths).  For (start, stop) data a second index 
orders the entry times.
<<coxme-setup>>=
    if (ncol(y) ==3) {
	if (length(strata) ==0) {
	    sorted <- cbind(order(-y[,2], y[,3]), 
			    order(-y[,1]))
	    newstrat <- n
	    }
	else {
	    sorted <- cbind(order(strata, -y[,2], y[,3]),
			    order(strata, -y[,1]))
	    newstrat  <- cumsum(table(strata))
	    }
	status <- y[,3]
        ofile <-  'agfit6b'
        rfile <-  'agfit6d'
        coxfitfun<- survival:::agreg.fit
        }
    else {
	if (length(strata) ==0) {
	    sorted <- order(-y[,1], y[,2])
	    newstrat <- n
	    }
	else {
	    sorted <- order(strata, -y[,1], y[,2])
	    newstrat <-  cumsum(table(strata))
	    }
	status <- y[,2]
        ofile <- 'coxfit6b' # fitting routine
        rfile <- 'coxfit6d' # refine.n routine
        coxfitfun <- survival:::coxph.fit
        }
@ 

The last step of the setup is to do an initial fit.  
We want two numbers: the loglik for a  no-random-effects and
initial-values-for-fixed (usually 0)
fit, and that for the best fixed effects fit.
The first is the NULL model loglik for the fit as a whole, the second
is used to scale the logliklihood during iteration, the [[fit0]] parameter
in the [[logfun]] function.
The easiest way to get these is from an ordinary [[coxph]] call.
Most coxph calls converge in 3-4 iterations.  The default value
for [[control$inner.iter]] is [[Quote(fit0$iter +1)]] to avoid disaster in the
case of a `hard' baseline model. We need to evaluate the expression  %'`
after fit0 is known.
If all values of $\theta$ are fixed, then the only thing we will
use from fit0 is the loglik.
Note that if there are no covariates or only an offset term, then 
the returned log-likelihood is of length 1, not 2.
a null model.
<<null-fit>>=
if (is.null(ifixed) ) ifixed <- rep(0., ncol(x))
else if (length(ifixed) != ncol(x))
    stop("Wrong length for initial parameters of the fixed effects")

if (length(itheta)==0) itemp <- 0 else itemp <- control$iter.max
fit0 <- coxfitfun(x,y, strata=strata, 
                  offset=offset, init=ifixed, weights=weights,
                  method=ties, rownames=1:nrow(y),
                  control=coxph.control(iter.max=itemp))
loglik0 <- fit0$loglik[length(fit0$loglik)]  # in case of no covariates  
control$inner.iter <- eval(control$inner.iter)
@ 

\subsection{Doing the fit}
If there are any parameters to optimize over, we now do so.
Our last step before optimization is to set the starting value.
We will have inherited a list of possible starting values for each
parameter in [[istart]]; try all combinations and keep the best one.
<<coxme-fit>>=
<<define-logfun>>

ishrink <- 0.7  # arbitrary guess
init.coef <- c(rep(0., npenal), scale*fit0$coef* ishrink)

if (length(itheta)==0) iter <- c(0,0)
else {
    <<coxme-gridsearch>>
@ 

This is set out as a block since it is also used in lmekin.
(Later, copied but not used due to different logfun).
<<coxme-gridsearch>>=
nstart <- sapply(itheta, length)
if (all(nstart==1)) theta <- unlist(itheta)  #one starting guess
else {
    #make a matrix of all possible starting estimtes
    testvals <- do.call(expand.grid, itheta)
    bestlog <- NULL
    for (i in 1:nrow(testvals)) {
        ll <- logfun(as.numeric(testvals[i,]), 
                     varlist, vparm, kfun, ntheta, ncoef, 
                     init=init.coef, loglik0, 
                     control$inner.iter, ofile)
        if (is.finite(ll)) {
            #ll calc can fail if someone picks a very bad starting guess
            if (is.null(bestlog) || ll < bestlog) {  
                # (optim is set up to minimize)
                bestlog <- ll
                theta <- as.numeric(testvals[i,])
            }
        }
    }
    if (is.null(bestlog))
        stop("No starting estimate was successful")
}
@ 
In the code below
[[optpar]] is a list of control parameters for the [[optim]] function,
which are defined in [[coxme.control]] and accessible for the user to
change, and [[logpar]] is a list of parameters that will be needed by
logfun.
In R the ones that are simple copies such as [[ofile]] would not need to
be included in the list since they are inherited with the environment,
however, I prefer to make such hidden arguments explicit.
<<coxme-fit>>=
    # Finally do the fit
    logpar <- list(varlist=varlist, vparm=vparm, 
                   ntheta=ntheta, ncoef=ncoef, kfun=kfun,
                   init=init.coef, fit0= loglik0,
                   iter=control$inner.iter,
                   ofile=ofile)
    mfit <- do.call('optim', c(list(par= theta, fn=logfun, gr=NULL), 
                           control$optpar, logpar))
    theta <- mfit$par
    iter <- mfit$counts[1] * c(1, control$inner.iter)
}
@

The optimization finds the best value of theta, but does not return
all the parameters we need from the fit.  So we make one more call.
This is essentially the ``inside'' of [[logfun]].  
The phrase [[c(ikmat@rmat,0)]] makes sure something is passed when
[[rmat]] is of length 0.

<<coxme-fit>>=
gkmat <- gchol(kfun(theta, varlist, vparm, ntheta, ncoef))
ikmat <- solve(gkmat)  #inverse of kmat, which is the penalty
fit <- .C(ofile,
          iter= as.integer(c(0, control$iter.max)),
          beta = as.double(c(rep(0., npenal), fit0$coef*scale)),
          loglik = double(2),
          as.double(ikmat@blocks),
          as.double(c(ikmat@rmat,0)),
          hdet = double(1))
ilik <- fit$loglik[2] -
  .5*(sum(log(diag(gkmat))) + fit$hdet)
iter[2] <- iter[2] + fit$iter[2]
@
              
\subsection{Finishing up}
\label{sect:final}
There are 6 tasks left to do
<<coxme-finish>>=
<<coxme-lastvar>>
<<coxme-rescale>>
<<coxme-df>>
<<refine>>  
.C("coxfit6e", as.integer(ncol(y)))  #release memory
<<create-output-list>>
@ 

The nexts section finishes up with the C code.
The first few lines reprise some variables
found in the C code but not before needed here.
It returns the score vector $u$, the 
sparse and dense portions of the Cholesky decomposition of the
Hessian matrix (h.b and h.r), the
inverse Hessian matrix (hi.b, hi.r), and the rank of the final 
solution.  These are needed to compute the variance matrix of the
estimates.
<<coxme-lastvar>>=
nfrail <- nrow(ikmat)  #total number of penalized terms
nsparse <- sum(ikmat@blocksize)
nvar2  <- nvar + (nfrail - nsparse)  # total number of non-sparse coefs
nvar3  <- as.integer(nvar + nfrail)  # total number of coefficients
btot   <- length(ikmat@blocks)

fit3 <- .C('coxfit6c',
               u    = double(nvar3),
               h.b  = double(btot),
               h.r  = double(nvar2*nvar3),
               hi.b = double(btot),
               hi.r = double(nvar2*nvar3),
               hrank= integer(1),
               as.integer(ncol(y))
               )
@ 

Now create the Hessian and inverse Hessian matrices; the latter of
these is the variance matrix.  
The C code had centered and rescaled all $X$ matrix coefficients
so we need to undo that scaling. 
First we deal with a special case, if there are only sparse terms
then [[hmat]] and [[hinv]] have only a block-diagonal component.
(This happens more often than you might think, a random per-subject
intercept for instance.)
<<coxme-rescale>>=
    if (nvar2 ==0) {
        hmat <- new('gchol.bdsmatrix', Dim=c(nvar3, nvar3),
                    blocksize=ikmat@blocksize, blocks=fit3$h.b,
                    rmat=matrix(0,0,0), rank=fit3$hrank,
                    Dimnames=list(NULL, NULL))
        hinv <- bdsmatrix(blocksize=ikmat@blocksize, blocks=fit3$hi.b)
        }
@ 
And now three cases: no $X$ variables, a single $X$, or multiple $X$
variables.
Assume there are $p$=nvar variables and let [[V]] be the lower $p \times p$
portion of the [[nvar3]] by [[nvar2]] $R$ matrix,
and $S$ = [[diag(scale]]
be the rescaling vector.
$X$ was replaced by $X S^{-1}$ before computation.
For the Hessian, we want to replace $V$ with $SVS$ and for the
inverse hessian with $S^{-1}V S^{-1}$.
The matrix [[hmat]] is however a Cholesky decomposition of the
hessian $H=LDL'$ where $L$ is lower triangular with ones on the     %'
diagonal and $D$ is diagonal; $D$ is kept on the diagonal of $V$ and
$L$ below the diagonal.
A little algebra shows that we want to replace $D$ (the diagonal of $L$)
with $S^2D$ and $L$ with $SLS^{-1}$.
<<coxme-rescale>>=
    else {
        rmat1 <- matrix(fit3$h.r, nrow=nvar3)
        rmat2 <- matrix(fit3$hi.r, nrow=nvar3)
        if (nvar ==1 ) {
            rmat1[nvar3,] <- rmat1[nvar3,]/scale
            rmat2[nvar3,] <- rmat2[nvar3,]/scale
            rmat1[,nvar2] <- rmat1[,nvar2]*scale
            rmat2[,nvar2] <- rmat2[,nvar2]/scale
            rmat1[nvar3,nvar2] <- rmat1[nvar3,nvar2]*scale^2
            u <- fit3$u  # the efficient score vector U
            u[nvar3] <- u[nvar3]*scale
            }
        else if (nvar >1) {
            temp <- seq(to=nvar3, length=length(scale))
            u <- fit3$u
            u[temp] <- u[temp]*scale
            rmat1[temp,] <- (1/scale)*rmat1[temp,] #multiply rows* scale
            rmat2[temp,] <- (1/scale)*rmat2[temp,] 

            temp <- temp-nsparse          #multiply cols
            rmat1[,temp] <- rmat1[,temp] %*% diag(scale)
            rmat2[,temp] <- rmat2[,temp] %*% diag(1/scale)
            temp <- seq(length=length(scale), to=length(rmat1), by=1+nvar3)
            rmat1[temp] <- rmat1[temp]*(scale^2)    #fix the diagonal
            }
        hmat <- new('gchol.bdsmatrix', Dim=c(nvar3, nvar3),
                    blocksize=ikmat@blocksize, blocks=fit3$h.b,
                    rmat= rmat1, rank=fit3$hrank,
                    Dimnames=list(NULL, NULL))
        hinv <- bdsmatrix(blocksize=ikmat@blocksize, blocks=fit3$hi.b,
                          rmat=rmat2)
        }
@             

Now for the degrees of freedom, using formula 5.16 of Therneau and Grambsch.
First we have a small utility function to compute the ${\rm trace}(AB)$ where
$A$ and $B$ are bdsmatrix objects.  
For ordinary matrices this is the sum of the element-wise product of $A$ and
$B'$, but we have to account for the fact that bdsmatrix objects only %'
keep the lower diagonal of the block portion. 
We need the diagonal sum + 2 times the off-diagonal
sum.  
<<coxme-df>>=
traceprod <- function(H, P) {
    #block-diagonal portions will  match in shape
    nfrail <- nrow(P)  #penalty matrix
    nsparse <- sum(P@blocksize)
    if (nsparse >0) {
        temp1 <- 2*sum(H@blocks * P@blocks) -
                 sum(diag(H)[1:nsparse] * diag(P)[1:nsparse])
        }
    else temp1 <- 0
    
    if (length(P@rmat) >0) {
        #I only want the penalized part of H
        rd <- dim(P@rmat)
        temp1 <- temp1 + sum(H@rmat[1:rd[1], 1:rd[2]] * P@rmat)
        }
    temp1
    }

df <- nvar + (nfrail - traceprod(hinv, ikmat))
@ 

And last, put together the output structure.
<<create-output-list>>=
idf <- nvar + sum(ntheta)
fcoef <- fit$beta[1:nfrail]
penalty <- sum(fcoef * (ikmat %*% fcoef))/2

if (nvar > 0) {
    out <- list(coefficients = fit$beta[-(1:nfrail)]/scale, frail=fcoef, 
         theta=theta, penalty=penalty,
         loglik=c(fit0$log[1], ilik, fit$log[2]), variance=hinv,
         df=c(idf, df), hmat=hmat, iter=iter, control=control,
         u=u, means=means, scale=scale)
    }
else out <- list(coefficients=NULL, frail=fcoef, 
                 theta=theta, penalty=penalty,
          loglik=c(fit0$log[1], ilik, fit$log[2]), variance=hinv,
          df=c(idf, df), hmat=hmat, iter=iter, control=control,
          u=fit3$u, means=means, scale=scale)    

if (refine.n>0) {
    out$refine <- r.correct
    #The next line can be turned on for detailed tests in refine.R
    #  The feature is not documented in the manual pages, only
    #  here.
    if (control$refine.detail)
        out$refine.detail <-list(loglik=rfit$loglik, bmat=bmat2, tdens=tdens,
                                penalty1=penalty1, penalty2=penalty2,
                                gdens=gdens, errhat=errhat, gkmat=gkmat)
    }
out
}
@ 
